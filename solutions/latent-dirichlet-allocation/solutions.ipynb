{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation\n",
    "\n",
    "## Introduction\n",
    "\n",
    "We're going to use Latent Dirichlet Allocation to do topic modeling on a bunch of articles from horror movies and paranormal events. The first goal is to discern what are the distinct topics within the dataset, and what features describe them. Then we'll write a function that takes a given article and returns the most similar articles\n",
    "\n",
    "## Basic\n",
    "### Part 1: Prepare data\n",
    "\n",
    "1. Load the data from `spooky_wikipedia.csv`. Since this is a Wikipedia dump, there are some pages (such as lists) that we're not interested in, so remove those. There are also some pages that have no text, so remove those as well. There's about 24,000 articles right now so take a smaller sample of that to start with (~1000). When you take a sample, pay attention to the indices as they might not look like you expect.\n",
    "\n",
    "    Hint: the `title` contains information about whether the page is a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/spooky_wikipedia.csv', index_col=0)\n",
    "df = df.iloc[:1000]\n",
    "list_articles = df.title.str.lower().str.contains('list of')\n",
    "df = df[~list_articles]\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Vectorize the corpus. Note that LDA generally does not take a TF-IDF matrix, but a bag-of-words vector (you can use sklearn's <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\">count vectorizer</a>). You can start with the default stopwords, but you'll probably want to update those later. We'll tune some of these other hyperparameters later but start with max_df = 0.85, min_df=2 and max_features=1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tf_vectorizer = CountVectorizer(stop_words='english', max_df=0.85, min_df=2, max_features=1000)\n",
    "word_vec = tf_vectorizer.fit_transform(df.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Build LDA model\n",
    "3. Create an <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\">LDA instance </a> and think about what each of the parameters mean. In our use case, what does n_components represent? How do we input our alpha and beta priors? Use the 'online' learning method and n_jobs=-1 (all cores) or -2 (all cores but one) to speed up your processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(learning_method='online', n_jobs=-2, random_state=1659)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Fit the LDA model on your vectorized corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(learning_method='online', n_jobs=-2,\n",
       "                          random_state=1659)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.fit(word_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Examine the generated topics. what does lda.components_ represent? How do we determine the most important features in a topic? Write a function that takes the most important features for each topic in lda.components_, then uses the feature names from the vectorizer to print out the most important words for each topic. What do you think each topic describes? Try adding some words to your stopwords to make your categories more specific to spooky topics and less to wikipedia topics.\n",
    "\n",
    "> Checkpoint 1: Nice work; you've learned how to fit an LDA model and examine the topics to gain an intuitive understanding of the latent associations in a set of documents.\n",
    "\n",
    "\n",
    "Helpful hint: if you don't want to keep fitting your vectorizer and lda model over and over again, you can persist them (save them to a file) with joblib (similar to pickle but optimized for large data)\n",
    "\n",
    "```python\n",
    "    joblib.dump(lda, 'lda_model.joblib')\n",
    "    joblib.dump(vectorizer, 'tf_vec.joblib')\n",
    "    lda = joblib.load('lda_model.joblib')\n",
    "    tf_vectorizer = joblib.load('tf_vec.joblib')\n",
    "    # It's that easy!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tf_vec.joblib']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(lda, 'lda_model.joblib')\n",
    "joblib.dump(tf_vectorizer, 'tf_vec.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "lda = joblib.load('lda_model.joblib')\n",
    "tf_vectorizer = joblib.load('tf_vec.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lda.components_` represents the distributions of words in each generated topic, where rows are topics and columns are words. The most important features in a topic are the words with the highest values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.13109778,  6.08632059,  7.15945968, ..., 21.56583322,\n",
       "         0.73597785,  4.9738055 ],\n",
       "       [ 0.10769533,  6.82274724,  2.24793448, ..., 12.8841689 ,\n",
       "         0.14241819,  2.75489192],\n",
       "       [ 7.76355189,  0.61276257,  0.12555321, ..., 24.1864745 ,\n",
       "         4.02857495,  3.93901932],\n",
       "       ...,\n",
       "       [ 0.256852  ,  0.63753218,  0.10746563, ..., 10.77513433,\n",
       "         0.11253416,  4.89832834],\n",
       "       [ 0.3887244 ,  0.1027463 ,  0.10192989, ...,  0.10292673,\n",
       "         0.39191822,  0.11213927],\n",
       "       [21.82286025,  5.21510422, 11.5115133 , ..., 51.65488355,\n",
       "        17.05453642,  8.84890863]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_topic_features(model, feature_names, num_features=10):\n",
    "    sorted_topics = feature_names[model.components_.argsort(axis=1)[:, ::-1][:, :num_features]]\n",
    "    return sorted_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['series', 'star', 'character', 'television', 'trek', 'wars',\n",
       "        'season', 'fictional', 'episode', 'franchise'],\n",
       "       ['known', 'war', 'world', 'greek', 'water', 'castle', 'states',\n",
       "        'south', 'light', 'black'],\n",
       "       ['house', 'king', 'century', 'known', 'play', 'james', 'opera',\n",
       "        'published', 'story', 'work'],\n",
       "       ['character', 'comic', 'game', 'fictional', 'book', 'published',\n",
       "        'story', 'books', 'man', 'universe'],\n",
       "       ['term', 'used', 'century', 'magic', 'religious', 'word', 'world',\n",
       "        'people', 'human', 'spiritual'],\n",
       "       ['buffy', 'harry', 'music', 'series', 'vampire', 'angel',\n",
       "        'slayer', 'potter', 'born', 'president'],\n",
       "       ['film', 'films', 'american', 'horror', 'best', 'directed',\n",
       "        'released', 'novel', 'award', 'fiction'],\n",
       "       ['god', 'greek', 'hebrew', 'jesus', 'goddess', 'book',\n",
       "        'according', 'wicca', 'jewish', 'king'],\n",
       "       ['loa', 'vodou', 'haitian', 'sun', 'moon', 'french', 'baron',\n",
       "        'sailor', 'ghede', 'tibetan'],\n",
       "       ['church', 'society', 'known', 'saint', 'new', 'christian',\n",
       "        'movement', 'united', 'religious', 'scientology']], dtype='<U16')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "feature_names = np.array(tf_vectorizer.get_feature_names())\n",
    "top_topic_features(lda, feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ENGLISH_STOP_WORDS.union({'film', 'directed', 'fictional', \n",
    "                                       'work', 'books', 'released', \n",
    "                                       'written', 'born', 'characters', \n",
    "                                       'television', 'episodes', \n",
    "                                       'director', 'novel', 'story', \n",
    "                                       'book', 'list', 'element', \n",
    "                                       'redirect', 'starring', \n",
    "                                       'fiction', 'story', 'produced', \n",
    "                                       'novel', 'based', 'character', \n",
    "                                       'game', 'comic', 'television', \n",
    "                                       'animated', 'tv', 'series', \n",
    "                                       'redirects', 'mentions', \n",
    "                                       'locations'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['star', 'trek', 'version', 'universe', 'created', 'wicca',\n",
       "        'space', 'appeared', 'time', 'role'],\n",
       "       ['films', 'american', 'best', 'time', 'award', 'million',\n",
       "        'awards', 'horror', 'received', 'academy'],\n",
       "       ['church', 'saint', 'christian', 'loa', 'catholic', 'roman',\n",
       "        'vodou', 'jesus', 'known', 'pope'],\n",
       "       ['wars', 'buffy', 'season', 'star', 'angel', 'stories', 'novels',\n",
       "        'tale', 'appears', 'episode'],\n",
       "       ['term', 'used', 'world', 'magic', 'century', 'word', 'human',\n",
       "        'ancient', 'religious', 'greek'],\n",
       "       ['god', 'king', 'hebrew', 'end', 'century', 'greek', 'jewish',\n",
       "        'house', 'bible', 'bc'],\n",
       "       ['scientology', 'church', 'hubbard', 'mind', 'halloween', 'salem',\n",
       "        'witch', 'astrology', 'jack', 'dianetics'],\n",
       "       ['horror', 'dead', 'dawn', 'black', 'baron', 'creature',\n",
       "        'campbell', 'ghede', 'evil', 'ring'],\n",
       "       ['known', 'united', 'society', 'american', 'states', 'years',\n",
       "        'war', 'later', 'world', 'new'],\n",
       "       ['play', 'godzilla', 'james', 'opera', 'king', 'japan',\n",
       "        'japanese', 'kong', 'christmas', 'sailor']], dtype='<U16')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vectorizer_2_5 = CountVectorizer(stop_words=stop_words, max_df=0.85, min_df=2, max_features=1000)\n",
    "word_vec_2_5 = tf_vectorizer_2_5.fit_transform(df.text)\n",
    "lda_2_5 = LatentDirichletAllocation(learning_method='online', n_jobs=-2, random_state=1659)\n",
    "lda_2_5.fit(word_vec_2_5)\n",
    "joblib.dump(lda_2_5, 'lda_model_2-5.joblib')\n",
    "joblib.dump(tf_vectorizer_2_5, 'tf_vec_2-5.joblib')\n",
    "lda_2_5 = joblib.load('lda_model_2-5.joblib')\n",
    "tf_vectorizer_2_5 = joblib.load('tf_vec_2-5.joblib')\n",
    "feature_names_2_5 = np.array(tf_vectorizer_2_5.get_feature_names())\n",
    "top_topic_features(lda_2_5, feature_names=feature_names_2_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced\n",
    "### Part 3: Build recommender\n",
    "\n",
    "6. Let's now work on creating a function that will take the name of an article and return the names of n articles most closely related to it. First we need to turn our vectorized corpus into an array of topic probabilities for each document. Which method of our model will return this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_article_idx(df, article_title):\n",
    "    return df.title[df.title == article_title].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_article_idx(df, 'Alchemy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(model, vectorizer, text):\n",
    "    if type(text) == str:\n",
    "        text = [text]\n",
    "    vec_text = vectorizer.transform(text)\n",
    "    doc_probs = model.transform(vec_text)\n",
    "    return doc_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00027375e-03, 1.00015114e-03, 1.00027679e-03, ...,\n",
       "        1.00006104e-03, 1.00020069e-03, 1.00008565e-03],\n",
       "       [1.01025954e-03, 7.98591683e-01, 1.01020428e-03, ...,\n",
       "        1.01020424e-03, 1.93325920e-01, 1.01040307e-03],\n",
       "       [7.14363731e-04, 7.14370961e-04, 7.14493125e-04, ...,\n",
       "        6.29865513e-03, 8.59948227e-01, 7.14362348e-04],\n",
       "       ...,\n",
       "       [5.99897115e-01, 4.54630067e-02, 2.38106583e-03, ...,\n",
       "        2.73231553e-02, 2.38128048e-03, 2.38106076e-03],\n",
       "       [9.83016919e-01, 1.88688948e-03, 1.88692015e-03, ...,\n",
       "        1.88702352e-03, 1.88702719e-03, 1.88688332e-03],\n",
       "       [4.76230503e-03, 4.76202272e-03, 9.57139194e-01, ...,\n",
       "        4.76205530e-03, 4.76221006e-03, 4.76221696e-03]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_proba(lda_2_5, tf_vectorizer_2_5, df.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Next, given a certain article, we need to compute the distance between this and every other document. sklearn.metrics.pairwise has great functions for cosine distance and euclidean distances here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances, euclidean_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Use cosine distance to create a vector that contains the distance from our document to every other document. Use argsort to determine the closest top 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_distance(doc_index, doc_probs, distance_func=cosine_distances, num_documents=10):\n",
    "    distances = distance_func(doc_probs[doc_index].reshape(1, -1), doc_probs).ravel()\n",
    "    return distances.argsort()[:num_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0, 214, 387, 361,  54,  61, 372, 901,  55,  98], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_index = find_article_idx(df, 'Alchemy')\n",
    "doc_probs = predict_proba(lda_2_5, tf_vectorizer_2_5, df.text)\n",
    "sort_by_distance(doc_index, doc_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Now we have an array that contains the indices of all of the most similar articles, we're almost there! Write a function that takes this array and returns the name of the input article as well as its most similar articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_document_titles(sorted_distances, titles):\n",
    "    name_array = titles.iloc[sorted_distances]\n",
    "    return {name_array.iloc[0]: name_array.iloc[1:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Alchemy': 214               Shamanism\n",
       " 388            Magic square\n",
       " 361    Magic (supernatural)\n",
       " 54               Dalai Lama\n",
       " 61               Divination\n",
       " 372    Dream interpretation\n",
       " 935    Pow-wow (folk magic)\n",
       " 55                    Demon\n",
       " 98                 Grimoire\n",
       " Name: title, dtype: object}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_index = find_article_idx(df, 'Alchemy')\n",
    "doc_probs = predict_proba(lda_2_5, tf_vectorizer_2_5, df.text)\n",
    "article_similarities = sort_by_distance(doc_index, doc_probs)\n",
    "find_closest_document_titles(article_similarities, df.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Checkpoint 2: Congratulations! You've just created a very useful recommender using LDA. This is a practical use-case; websites often use a similar approach to determine the articles for recommended reading that appear below the article text or in sidebars.\n",
    "\n",
    "### Part 4: Evaluation and make improvements\n",
    "10. Do your recommendations make sense? Try changing hyperparameters of your count vectorizer and your LDA model to try to improve them!\n",
    "I had pretty good results using the full dataset and these parameters:\n",
    "```python\n",
    "    lda = LatentDirichletAllocation(n_components = 20, learning_offset =50., verbose=1,\n",
    "                                    doc_topic_prior=0.9, topic_word_prior= 0.9,\n",
    "                                    n_jobs=-1, learning_method = 'online')\n",
    "    tf_vectorizer =  CountVectorizer(max_df=0.85, min_df=2, max_features = 1000,\n",
    "                                    stop_words=stop_words, ngram_range = (1,3))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 10\n",
      "iteration: 2 of max_iter: 10\n",
      "iteration: 3 of max_iter: 10\n",
      "iteration: 4 of max_iter: 10\n",
      "iteration: 5 of max_iter: 10\n",
      "iteration: 6 of max_iter: 10\n",
      "iteration: 7 of max_iter: 10\n",
      "iteration: 8 of max_iter: 10\n",
      "iteration: 9 of max_iter: 10\n",
      "iteration: 10 of max_iter: 10\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components = 20, learning_offset =50., verbose=1,\n",
    "                                doc_topic_prior=0.9, topic_word_prior= 0.9,\n",
    "                                n_jobs=-1, learning_method = 'online', random_state=1659)\n",
    "tf_vectorizer =  CountVectorizer(max_df=0.85, min_df=2, max_features = 1000,\n",
    "                                stop_words=stop_words, ngram_range = (1,3))\n",
    "\n",
    "word_vec_4 = tf_vectorizer.fit_transform(df.text)\n",
    "lda.fit(word_vec_4)\n",
    "joblib.dump(lda, 'lda_model_4.joblib')\n",
    "joblib.dump(tf_vectorizer, 'tf_vec_4.joblib')\n",
    "lda = joblib.load('lda_model_4.joblib')\n",
    "tf_vectorizer = joblib.load('tf_vec_4.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_closest_features(model, vectorizer, df, article_title):\n",
    "    doc_index = find_article_idx(df, article_title)\n",
    "    doc_probs = predict_proba(model, vectorizer, df.text)\n",
    "    article_similarities = sort_by_distance(doc_index, doc_probs)\n",
    "    return find_closest_document_titles(article_similarities, df.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Alchemy': 408    Fortune-telling\n",
       " 181             Occult\n",
       " 70         Eschatology\n",
       " 4              Animism\n",
       " 186         Pythagoras\n",
       " 83       Faith healing\n",
       " 388       Magic square\n",
       " 214          Shamanism\n",
       " 103         Gnosticism\n",
       " Name: title, dtype: object}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_closest_features(lda, tf_vectorizer, df, 'Alchemy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Chupacabra': 857                   Majestic 12\n",
       " 824                 Donald Keyhoe\n",
       " 693                          Vayu\n",
       " 501                    Buckriders\n",
       " 277    Unidentified flying object\n",
       " 772                    Candy corn\n",
       " 702               Newton, Alabama\n",
       " 951                   Foo fighter\n",
       " 33      Earth (classical element)\n",
       " Name: title, dtype: object}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_closest_features(lda, tf_vectorizer, df, 'Chupacabra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ghost': 719             Damnation\n",
       " 235                  Soul\n",
       " 422            Necromancy\n",
       " 920                  Omen\n",
       " 550              Caduceus\n",
       " 811     Western astrology\n",
       " 889    Magic and religion\n",
       " 874               Saṃsāra\n",
       " 685              Orunmila\n",
       " Name: title, dtype: object}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_closest_features(lda, tf_vectorizer, df, 'Ghost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Since we don't have traditional error metrics like we would in a supervised learning approach, it's hard to tune these hyperparameters in the same way. We can, however, use log-likelihood as a scoring function for the LDA model. We split our data, train our model, and then determine the likelihood that that our model of the documents could have generated the unseen text. The higher this value, the \"better\" we have modeled our corpus.\n",
    "Using sklearn <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\">GridSearchCV</a> or <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\">RandomizedSearchCV</a> tune the number of topics using cross validation on log-loss(equivalent to negative log-likelihood; log-loss is the default scorer for the sklearn LDA model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "\n",
    "X_train, X_test = train_test_split(word_vec_4, random_state=1659)\n",
    "\n",
    "params = {'n_components': [2, 5, 10, 50, 75, 100, 200], \n",
    "          'doc_topic_prior': stats.uniform(),\n",
    "          'topic_word_prior': stats.uniform(),\n",
    "          'learning_offset': stats.uniform(10, 90)}\n",
    "lda.set_params(**{'verbose': 0, 'n_jobs': -2})\n",
    "lda_cv = RandomizedSearchCV(lda, params, n_iter=1, n_jobs=-2)\n",
    "\n",
    "results = {'mean_test_score': [],\n",
    "'std_test_score': [],\n",
    "'params': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 2\n",
    "\n",
    "for _ in range(n_iter):\n",
    "    lda_cv.fit(X_train)\n",
    "    results['mean_test_score'].append(lda_cv.cv_results_['mean_test_score'][0])\n",
    "    results['std_test_score'].append(lda_cv.cv_results_['std_test_score'][0])\n",
    "    results['params'].append(lda_cv.cv_results_['params'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-58863.411374</td>\n",
       "      <td>4363.600763</td>\n",
       "      <td>{'doc_topic_prior': 0.036429455021845025, 'lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-76467.160520</td>\n",
       "      <td>4633.829837</td>\n",
       "      <td>{'doc_topic_prior': 0.7833607791416924, 'learn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_score  std_test_score  \\\n",
       "0    -58863.411374     4363.600763   \n",
       "1    -76467.160520     4633.829837   \n",
       "\n",
       "                                              params  \n",
       "0  {'doc_topic_prior': 0.036429455021845025, 'lea...  \n",
       "1  {'doc_topic_prior': 0.7833607791416924, 'learn...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_csv('lda_tuning.csv', index=False)\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra credit\n",
    "### Part 4: Classes\n",
    "\n",
    "Put this all into a class for easy usage!\n",
    "\n",
    "see `solution.py`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
