{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "255d6f5c4ad1a343cfd66d9bd5684b331c68c83008bd168b172f50c2faa51e0a"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Part 3: Tuning and Model Comparison"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, LeaveOneOut, KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "source": [
    "Your result will look like the following."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "%%capture\n",
    "'''\n",
    "tuning naive bayes...\n",
    "alpha  score\n",
    " 0.00  0.146634\n",
    " 0.02  0.837728\n",
    " 0.04  0.840571\n",
    " 0.06  0.839864\n",
    " 0.08  0.834170\n",
    " 0.10  0.834167\n",
    " 0.20  0.804260\n",
    " 0.30  0.787169\n",
    " 0.40  0.767946\n",
    " 0.50  0.755127\n",
    " 0.60  0.740888\n",
    " 0.70  0.731638\n",
    " 0.80  0.720959\n",
    " 0.90  0.704589\n",
    " 1.00  0.691061\n",
    " 1.10  0.683228\n",
    " 1.20  0.674684\n",
    "running models...\n",
    "                Name   Score TrainTime  TestTime\n",
    "       Random Forest   0.730      1.49      0.06\n",
    "       Decision Tree   0.608     16.31      0.03\n",
    "                 kNN   0.798      1.89     24.69\n",
    "         Naive Bayes   0.750      0.59      0.06\n",
    "                 SVM   0.807     97.71     34.25\n",
    "            Logistic   0.864      2.07      0.39\n",
    "'''"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    newsgroups_train = fetch_20newsgroups(subset='train', categories=['alt.atheism', \n",
    "    'sci.space', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns'])\n",
    "    data = newsgroups_train.data\n",
    "    labels = newsgroups_train.target\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(labels)\n",
    "    return np.array(data), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_naive_bayes(data, y):\n",
    "    print(\"tuning naive bayes...\")\n",
    "    kfold = KFold(5)\n",
    "    alphas = np.concatenate((np.arange(0, 0.1, 0.02), np.arange(.1, 1.3, 0.1)))\n",
    "    scores = defaultdict(list)\n",
    "    for train_index, test_index in kfold.split(data):\n",
    "        data_train, data_test = data[train_index], data[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        tfidf = TfidfVectorizer()\n",
    "        X_train = tfidf.fit_transform(data_train)\n",
    "        print(X_train.shape)\n",
    "        X_test = tfidf.transform(data_test)\n",
    "        for alpha in alphas:\n",
    "            nb = MultinomialNB(alpha=alpha)\n",
    "            nb.fit(X_train, y_train)\n",
    "            scores[alpha].append(nb.score(X_test, y_test))\n",
    "\n",
    "    print(\"alpha  score\")\n",
    "    for alpha in alphas:\n",
    "        print(\" %.2f  %f\" % (alpha, np.average(scores[alpha])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_models(data, y):\n",
    "    data_train, data_test, y_train, y_test = train_test_split(data, y)\n",
    "\n",
    "    tfidf = TfidfVectorizer()\n",
    "    X_train = tfidf.fit_transform(data_train).toarray()\n",
    "    X_test = tfidf.transform(data_test).toarray()\n",
    "\n",
    "    print(\"running models...\")\n",
    "    models = [(\"Random Forest\", RandomForestClassifier()),\n",
    "              (\"Decision Tree\", DecisionTreeClassifier()),\n",
    "              (\"kNN\", KNeighborsClassifier()),  \n",
    "              (\"Naive Bayes\", MultinomialNB()),\n",
    "              (\"SVM\", OneVsRestClassifier(SVC())),\n",
    "              (\"Logistic\", OneVsRestClassifier(LogisticRegression()))]\n",
    "\n",
    "    print(\"%20s %7s %9s %9s\" % (\"Name\", \"Score\", \"TrainTime\", \"TestTime\"))\n",
    "\n",
    "    for name, model in models:\n",
    "        start = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        trained = time.time()\n",
    "        score = model.score(X_test, y_test)\n",
    "        tested = time.time()\n",
    "\n",
    "        # Silly stuff to make it print nicely\n",
    "        print(\"%20s   %.3f %9s %9s\" % (name, score,\n",
    "                                       str(round(trained - start, 2)),\n",
    "                                       str(round(tested - trained, 2))))"
   ]
  },
  {
   "source": [
    "### Run the whole program"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tuning naive bayes...\n",
      "(858, 20415)\n",
      "(858, 20019)\n",
      "(858, 20094)\n",
      "(859, 20119)\n",
      "(859, 20207)\n",
      "alpha  score\n",
      " 0.00  0.991619\n",
      " 0.02  0.994414\n",
      " 0.04  0.994414\n",
      " 0.06  0.994414\n",
      " 0.08  0.995344\n",
      " 0.10  0.995344\n",
      " 0.20  0.995344\n",
      " 0.30  0.995344\n",
      " 0.40  0.995344\n",
      " 0.50  0.995344\n",
      " 0.60  0.995344\n",
      " 0.70  0.995344\n",
      " 0.80  0.995344\n",
      " 0.90  0.995344\n",
      " 1.00  0.995344\n",
      " 1.10  0.995344\n",
      " 1.20  0.995344\n",
      "running models...\n",
      "                Name   Score TrainTime  TestTime\n",
      "       Random Forest   0.974      2.31      0.07\n",
      "       Decision Tree   0.929      0.94      0.01\n",
      "                 kNN   0.989      1.45      9.18\n",
      "         Naive Bayes   0.993      0.15      0.01\n",
      "                 SVM   0.985     19.81      6.17\n",
      "            Logistic   0.981      0.64      0.01\n"
     ]
    }
   ],
   "source": [
    "data, y = get_data()\n",
    "tune_naive_bayes(data, y)\n",
    "run_models(data, y)"
   ]
  }
 ]
}