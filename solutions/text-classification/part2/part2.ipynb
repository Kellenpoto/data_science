{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Part 2: Applications of tf-idf and cosine similarity"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "source": [
    "## Load the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select these 4 groups\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "# 1. For 4 groups (classes) of the 20newsgroups corpus (your choice), find the 10 most important words by:\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, random_state=42)"
   ]
  },
  {
   "source": [
    "##  Feature importances"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english',analyzer='word')\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data)"
   ]
  },
  {
   "source": [
    "1. For 4 groups (classes) of the 20newsgroups corpus (your choice), find the 10 most important words by:\n",
    "    * total tf-idf score\n",
    "    * average tf-idf score (average only over non-zero values)\n",
    "    * highest tf (only) score across corpus (try using `use_idf = False` in `TfidfVectorizer` )"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Let's build a reverse dictionary of the index-to-word mapping."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Average number of words per document."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "114.78072763028516"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "vectors.nnz / float(vectors.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_2_word = {}\n",
    "for word, idx in vectorizer.vocabulary_.items():\n",
    "   index_2_word[idx] = word"
   ]
  },
  {
   "source": [
    "Now, let's conver the sparse matrix tfidf representation to a regulary numpy array. Note that this may not be the optimal solution when your sparse matrix dimension is gigantic."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(2034, 33814)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "tfidf_numpy = vectors.toarray()\n",
    "tfidf_numpy.shape"
   ]
  },
  {
   "source": [
    "Sort the total tf-idf score according to the 1) total, 2) mean and 3) sum"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_score = tfidf_numpy.sum(axis=0)"
   ]
  },
  {
   "source": [
    "The mean score is a little bit tricky because we are only allowed to average non-zero values. Check this stackoverflow [answer](https://stackoverflow.com/questions/38542548/numpy-mean-of-nonzero-values)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_score = np.true_divide(tfidf_numpy.sum(axis=0),(tfidf_numpy!=0).sum(axis=0))"
   ]
  },
  {
   "source": [
    "The max score requires another instance of `vectorizer`. Since it only uses term frenquency, I am going to call it `tf_numpy`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_only_vectorizer = TfidfVectorizer(stop_words='english', use_idf = False, analyzer='word')\n",
    "tf_numpy = tf_only_vectorizer.fit_transform(newsgroups_train.data).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_2_word_tf_only = {}\n",
    "for word, idx in tf_only_vectorizer.vocabulary_.items():\n",
    "   index_2_word_tf_only[idx] = word"
   ]
  },
  {
   "source": [
    "### Do the two dicts agree?\n",
    "Yes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "index_2_word == index_2_word_tf_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score_tf_only = tf_numpy.max(axis=0)"
   ]
  },
  {
   "source": [
    "#### Now, let's use the index-to-word map to find the top 10.\n",
    "\n",
    "[How do I get indices of N maximum values in a NumPy array?](https://stackoverflow.com/questions/6910641/how-do-i-get-indices-of-n-maximum-values-in-a-numpy-array)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_2_words(idx, dict_ = index_2_word):\n",
    "    '''\n",
    "    idx is a list, return the corresponding words in the `dict_` as a sorted list\n",
    "    '''\n",
    "    return sorted([dict_.get(idx_,None) for idx_ in idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "By total score, most important 10 words are: ['article', 'com', 'edu', 'god', 'lines', 'organization', 'people', 'space', 'subject', 'writes']\n"
     ]
    }
   ],
   "source": [
    "top_k = 10\n",
    "total_score_index_10 = total_score.argsort()[-top_k:][::-1]\n",
    "print(\"By total score, most important 10 words are: {}\".format(idx_2_words(total_score_index_10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "By mean score, most important 10 words are: ['b12', 'dlb', 'enviroleague', 'kewageshig', 'landis', 'p_c', 'sphinx', 'stereoscopic', 'vonnegut', 'xxxx']\n"
     ]
    }
   ],
   "source": [
    "mean_score_index_10 = mean_score.argsort()[-top_k:][::-1]\n",
    "print(\"By mean score, most important 10 words are: {}\".format(idx_2_words(mean_score_index_10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "By max score, most important 10 words are: ['000', '___', 'ellipse', 'gb', 'jpeg', 'law', 'ra', 'space', 'tyre', 'xxxx']\n"
     ]
    }
   ],
   "source": [
    "top_k = 10\n",
    "max_score_index_10 = max_score_tf_only.argsort()[-top_k:][::-1]\n",
    "print(\"By max score, most important 10 words are: {}\".format(idx_2_words(max_score_index_10, index_2_word_tf_only)))"
   ]
  },
  {
   "source": [
    "2. Do the top 10 words change based on each of the different ranking methods?\n",
    "\n",
    "> Yes. \n",
    "\n",
    "3. Also do this for each category of article (each of the 20 newsgroups) and compare the top words of each. You should treat each category of newsgroup as a separate \"corpus\" for this question.\n",
    "\n",
    "> Left to the students as exercise. Check the names of the 20 corpus as below."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "list(fetch_20newsgroups(subset='train').target_names)"
   ]
  },
  {
   "source": [
    "#### Ranking\n",
    "\n",
    "You can use cosine similarity to rank the relevance of a document to a given search query using the following process:\n",
    "* Convert a search query into a feature vector, treat the query as a document in your corpus and apply tf-idf vectorizing to it.\n",
    "* Normalize your query vector and all of your document vectors (since documents are often much longer than a query)\n",
    "* Compute the cosine similarity between the search query and each of your documents\n",
    "* Rank the documents by their similarity score\n",
    "\n",
    "Sample queries are available in `data/queries.txt`.\n",
    "\n",
    "1. For each query, find the 3 most relevant articles from the 20 Newsgroups corpus."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We are still going to use the 4 categories we fetched in part 1 as example.\n",
    "\n",
    "#### 1. First, let's read the queries into a list."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/queries.txt\",\"r\") as fp:\n",
    "    queries = list(map(lambda x: x.strip(), fp.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['cheerleader nation tv show',\n",
       " 'budget rental cars',\n",
       " 'children who have died from moms postpartum depression',\n",
       " 'compaq presario notebook v5005us',\n",
       " 'boxed set of fruits basket',\n",
       " 'sun sentinal news paper',\n",
       " 'puerto rico economy',\n",
       " 'wireless networking',\n",
       " 'hidden valley ranch commercials',\n",
       " 'jimmy carter the panama canal']"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "queries"
   ]
  },
  {
   "source": [
    "#### 2. Now, we write a function to turn search query into (normalized) feature vector. We also want to normalize our `tfidf_numpy` array."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query2vec(single_query, normalized = True, vectorizer = vectorizer):\n",
    "    vector = vectorizer.transform([single_query]) # iterable as if it is a document\n",
    "    vector = vector.toarray().flatten() # flatten the 1 by N matrix to a simple vector\n",
    "    if normalized:\n",
    "        norm = np.linalg.norm(vector)\n",
    "        if norm == 0:\n",
    "            return vector\n",
    "        else:\n",
    "            return vector/norm\n",
    "    return vector"
   ]
  },
  {
   "source": [
    "# normalize our tfidf_numpy. Note that you want each row to be normalized, since each row represent a document.\n",
    "from sklearn.preprocessing import normalize as sk_normalizer\n",
    "tfidf_numpy_doc_normalized = sk_normalizer(tfidf_numpy,axis=1)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 21,
   "outputs": []
  },
  {
   "source": [
    "#### 3. For each query, find the top 3 most likely docs\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b)/(np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "source": [
    "#### For each query, we calculate the similarity scores with every document and print ouf the content."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Checking query: [cheerleader nation tv show]\n",
      "1471\n",
      "695\n",
      "1809\n",
      "Checking query: [budget rental cars]\n",
      "1399\n",
      "354\n",
      "1215\n",
      "Checking query: [children who have died from moms postpartum depression]\n",
      "899\n",
      "838\n",
      "2021\n",
      "Checking query: [compaq presario notebook v5005us]\n",
      "1264\n",
      "1008\n",
      "1530\n",
      "Checking query: [boxed set of fruits basket]\n",
      "1226\n",
      "264\n",
      "1494\n",
      "Checking query: [sun sentinal news paper]\n",
      "1246\n",
      "1238\n",
      "1927\n",
      "Checking query: [puerto rico economy]\n",
      "1382\n",
      "441\n",
      "1947\n",
      "Checking query: [wireless networking]\n",
      "665\n",
      "876\n",
      "813\n",
      "Checking query: [hidden valley ranch commercials]\n",
      "283\n",
      "1539\n",
      "482\n",
      "Checking query: [jimmy carter the panama canal]\n",
      "43\n",
      "1855\n",
      "1195\n"
     ]
    }
   ],
   "source": [
    "for idx, query in enumerate(queries):\n",
    "    # calculate the query vector\n",
    "    query_vector = query2vec(query)\n",
    "    print(\"Checking query: [{}]\".format(query))\n",
    "    cos_scores = [cosine_similarity(query_vector, tfidf_numpy_doc_normalized[i]) for i in range(tfidf_numpy_doc_normalized.shape[0])]\n",
    "    # find the top 3 maximal value's index\n",
    "    top_3_index = np.array(cos_scores).argsort()[-3:][::-1]\n",
    "    # print the contents to check intution agreement.\n",
    "    for index in top_3_index:\n",
    "        print(index)\n",
    "        # uncomment the following line to check the actual contents.\n",
    "        # print(newsgroups_train.data[index])"
   ]
  }
 ]
}