A 70-year-old woman visits a doctor complaining of abdominal pain. The 
doctor knows that there are only three potential causes of abdominal 
pain given the patient's medical history and that they cannot co-occur. 
They are presented below with their prevalence: 

partial small-bowel obstruction - 30%
peptic ulcer disease - 30% 
diverticulitis - 40%

Abdominal pain is a symptom in 70% of partial small-bowel obstructions, 
30% of peptic ulcer disease cases, and 50% of diverticulitis cases. 

What is the most likely condition the patient has? What is the 
calculated likelihood?

    `BO = {Bowel Obstruction}, PUD = {Peptic Ulcer Disease}, D = {Diverticulitis}`

    Calculate their probabilities.

    `P(BO) = 0.30, P(PUD) = 0.30 , P(D) = 0.40`

    `AP = {Abdominal Pain}`

    Find the probability that a patient having one of the three 
    conditions will have abdominal pain

    `P(AP|BO) = 0.70, P(AP|PUD) = 0.30, P(AP|D) = 0.50.`

    `P(AP) = P(AP|BO) * P(BO) + P(AP|PUD) * P(PUD) + P(AP|D) * P(D)`

        = 0.70 * 0.30 + 0.30 * 0.30 + 0.50 * 0.40
        
        = 0.50

    Use Bayes' theorem

    `P(BO|AP) = P(AP|BO) * P(BO) / P(AP) `
            `= (0.70 * .30) / 0.50 = 0.42`
    `P(PUD|AP) = P(AP|PUD) * P(PUD) / P(AP) `
            `= (0.30 * .30) / 0.50 = 0.18`
    `P(D|AP) = P(AP|D) * P(D) / P(AP) `
            `= (0.50 * .40) / 0.50 = 0.40`

    The patient is likeliest to have a partial small-bowel obstruction, 
    with a calculated likelihood of 42%.


Consider a corpus made up of the following four documents:

    Doc 1: Dogs like dogs more than cats.
    Doc 2: The dog chased the bicycle.
    Doc 3: The cat rode in the bicycle basket.
    Doc 4: I have a fast bicycle.

    We assume that we are lowercasing everything, stemming and removing stop 
    words and punctuation. These are the features you should have:

    ```dog, like, cat, chase, bicycle, ride, basket, fast```

    Use the unnormalized versions of TF and DF that you learned in class to 
    answer the following questions.

    * What is the term frequency vector for Document 1 (2 pts)?

      doc1_tf = [2, 1, 1, 0, 0, 0, 0, 0]

    * What is the document frequency vector for all the words in the corpus (1 pt)?

      doc_freq = [2, 1, 2, 1, 3, 1, 1, 1]


a) Describe the process of choosing the best k in k-Means. (2 pts; describe 2 methods)

    We examined three methods to choose the optimal number of clusters (k) in k-means: the elbow method, 
    the gap statistic and silhouette scores.
    One way to find which K is optimal is to increase K until you reach an elbow point in the plot 
    of within-cluster variation against the number of clusters. One could also use the GAP statistic or compute
    the silhouette coefficient for each K.  The GAP statistic is explicit about 
    how to choose K; the number of clusters that maximizes the GAP statistics is the optimal number of clusters.  
    
    Silhouette coefficients range from -1 to +1 and indicate how similar an observation is with its own cluster,
     with scores of +1 indicating the highest internal consistency. 
     The number of clusters for which most observations have a high silhouette coefficient is optimal.


    b) List 2 ways of computing the distance between clusters in hierarchical clustering. (1 pt.)

    Complete: Maximum pairwise dissimilarity between points in clusters
    Average: Average of pairwise dissimilarity between points in clusters
    Single: Minimum pairwise dissimilarity between points in clusters
    Centroid: Dissimilarity between centroids
    Ward: Minimum total within-cluster variance


In PCA utilizing eigen-decompostion, explain:
   
    a) How the principal components are calculated? (2 pts.)

    Principal components can be calculated using eigen-decomposition or with SVD.
    In the case of eigen-decomposition, the feature covariance/correlation matrix is decomposed into 
    a matrix of eigenvectors and a list of corresponding eigenvalues. The eigenvectors are the principal components.
    Eigenvalues indicate the magnitude of each of the eigenvectors, that is, the portion of 'variance' associated with each eigenvector.

    b) How to calculate the proportion of total variance explained by one principal
        component? (1 pt.)

    The variance explained by one component is its eigenvalue divided by the sum
    of all the eigenvalues.


The matrix below shows nutritional values for 5 food items.
    
                -------------------------------------------------------
                | Apple | Banana | Bell Pepper | Blue Crab | Broccoli |
    --------------------------------------------------------------------
    |Calories    |  130  |   110  |      25     |    100    |     45   |
    --------------------------------------------------------------------
    |Sodium      |    0  |     0  |      40     |    330    |     80   |
    --------------------------------------------------------------------
    |Potassium   |  260  |   450  |     220     |    300    |    460   |
    --------------------------------------------------------------------
    |Carbohydrate|   34  |    30  |       6     |      0    |      8   |
    --------------------------------------------------------------------
    |Vitamin A   |    2  |     2  |       4     |      0    |      6   |   
    --------------------------------------------------------------------
    |Vitamin C   |    8  |    15  |     190     |      4    |    220   |  
    --------------------------------------------------------------------

    Using sklearn's NMF class in its decomposition library:

    a) Provide code that would factorize this matrix into W and H matrices.  
    Paste the code into the space below.  Some starter code is provided (2 pts).

    # starter code
    import numpy as np
    from sklearn.decomposition import NMF

    nut_vals=np.array([[130, 110,  25, 100,  45],
                        [  0,   0,  40, 330,  80],
                        [260, 450, 220, 300, 460],
                        [ 34,  30,   6,   0,   8],
                        [  2,   2,   4,   0,   6],
                        [  8,  15, 190,   4, 220]])
    # your code below
    model = NMF(n_components=3, init='random', random_state=0)
    W = model.fit_transform(nut_vals)
    H = model.components_

    b) Describe why you used the number of latent topics you did in your NMF. (1 pt)

    Answers will vary.  An arguable answer is that inspection of the table above shows
    three groups (fruit, vegetables, meat) and so 3 was tried.  A more systematic
    approach is shown in nutritional_value.py where the reconstruction error
    was calculated for a range of components, and 3 components were
    in the "elbow" of low reconstruction error.


In the NMF described in question 4:   

    a) How would you check if the number of latent topics you picked was appropriate? (2 pts)

    Through matrix reconstruction error. As the number of latent topics increases reconstruction
    error will go down, but an "elbow" may occur that indicates that particular number of
    latent topics decreases reconstruction error by a large amount, and diminishing returns
    occur with more latent topics.  See nutritional_value.py.

    b) In the case of these nutritional values, why might NMF be more appropriate matrix
        factorization model than SVD? (1 pt)

    There is no such thing as a negative nutritional value.  It makes sense that nutritional
    topics should be composed of linear additions (not subtractions) of nutritional values.